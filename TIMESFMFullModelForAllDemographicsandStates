!git clone https://github.com/time-series-foundation-models/lag-llama/
cd /content/lag-llama
!pip install -r requirements.txt --quiet # this could take some time # ignore the errors displayed by colab
!huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir /content/lag-llama
!pip install datasets
from itertools import islice
from matplotlib import pyplot as plt
import matplotlib.dates as mdates
import torch
from gluonts.evaluation import make_evaluation_predictions
from gluonts.dataset.pandas import PandasDataset
from datasets import load_dataset
import pandas as pd
from lag_llama.gluon.estimator import LagLlamaEstimator
from itertools import islice
from matplotlib import pyplot as plt
import matplotlib.dates as mdates
import torch
from gluonts.evaluation import make_evaluation_predictions
from gluonts.dataset.pandas import PandasDataset
from datasets import load_dataset
import pandas as pd
from lag_llama.gluon.estimator import LagLlamaEstimator

# List of states
states = ["Alabama", "California", "Texas", "NewYork", "Hawaii", "Wyoming"]

# Dictionary to store datasets
datasets = {}
train_datasets={}

for state in states:
    # Load dataset for each state
    df = load_dataset(f"AdityaA44/MonthlyRacepopulation{state}", split='train').to_pandas()

    # Convert 'Date' to datetime format
    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')  # Adjust the format if necessary

    # Melt dataframe to long format
    df = df.melt(id_vars='Date', var_name='type_id', value_name='population')

    # Set 'Date' as the index
    df.set_index('Date', inplace=True)

    # Convert numerical columns to float32
    for col in df.columns:
        if df[col].dtype != 'object' and pd.api.types.is_string_dtype(df[col]) == False:
            df[col] = df[col].astype('float32')

    # Create the LagLlama dataset object
    dataset = PandasDataset.from_long_dataframe(df,
                                                target="population",
                                                item_id="type_id",
                                                freq="M")





    train_end=round(len(df)*1.0)
    train_dataset = PandasDataset.from_long_dataframe(df[:train_end],
                                                target="population",
                                                item_id="type_id",
                                                freq="M")
    train_datasets[state]=train_dataset

    # Store dataset in dictionary
    datasets[state] = dataset
    print(df)

def get_lag_llama_predictions(dataset, prediction_length, device, context_length=32, use_rope_scaling=False, num_samples=100):
    # Load model checkpoint
    ckpt = torch.load("lag-llama.ckpt", map_location=device)  # Ensure it's loaded to CPU

    estimator_args = ckpt["hyper_parameters"]["model_kwargs"]

    rope_scaling_arguments = {
        "type": "linear",
        "factor": max(1.0, (context_length + prediction_length) / estimator_args["context_length"]),
    }

    estimator = LagLlamaEstimator(
        ckpt_path="lag-llama.ckpt",
        prediction_length=prediction_length,
        context_length=context_length,

        # estimator args
        input_size=estimator_args["input_size"],
        n_layer=estimator_args["n_layer"],
        n_embd_per_head=estimator_args["n_embd_per_head"],
        n_head=estimator_args["n_head"],
        scaling=estimator_args["scaling"],
        time_feat=estimator_args["time_feat"],
        rope_scaling=rope_scaling_arguments if use_rope_scaling else None,

        batch_size=1,
        num_parallel_samples=num_samples,
        device=device,
    )

    lightning_module = estimator.create_lightning_module()
    transformation = estimator.create_transformation()
    predictor = estimator.create_predictor(transformation, lightning_module)

    forecast_it, ts_it = make_evaluation_predictions(
        dataset=dataset,
        predictor=predictor,
        num_samples=num_samples
    )
    forecasts = list(forecast_it)
    tss = list(ts_it)
    return forecasts, tss

prediction_length=12
context_length=64
num_samples=100
device="cpu"
ckpt = torch.load("lag-llama.ckpt", map_location=device)
estimator_args = ckpt["hyper_parameters"]["model_kwargs"]

estimator = LagLlamaEstimator(
        ckpt_path="lag-llama.ckpt",
        prediction_length=prediction_length,
        context_length=context_length,

        # distr_output="neg_bin",
        # scaling="mean",
        nonnegative_pred_samples=True,
        aug_prob=0,
        lr=5e-4,

        # estimator args
        input_size=estimator_args["input_size"],
        n_layer=estimator_args["n_layer"],
        n_embd_per_head=estimator_args["n_embd_per_head"],
        n_head=estimator_args["n_head"],
        time_feat=estimator_args["time_feat"],

        # rope_scaling={
        #     "type": "linear",
        #     "factor": max(1.0, (context_length + prediction_length) / estimator_args["context_length"]),
        # },

        batch_size=64,
        num_parallel_samples=num_samples,
        trainer_kwargs = {"max_epochs": 60,}, # <- lightning trainer arguments
    )
predictor = estimator.train(train_dataset, cache_data=True, shuffle_buffer_length=1000)
forecast_it, ts_it = make_evaluation_predictions(
        dataset=train_dataset,
        predictor=predictor,
        num_samples=num_samples
    )
from tqdm.autonotebook import tqdm

forecasts = list(tqdm(forecast_it, total=len(dataset), desc="Forecasting batches"))
tss = list(tqdm(ts_it, total=len(dataset), desc="Ground truth"))
from sklearn.metrics import mean_squared_error
import numpy as np

# Function to compute RMSE
def calculate_rmse(actual, predicted):
    return np.sqrt(mean_squared_error(actual, predicted))



# Plot results for each state
for state in states:
    if state in datasets:
        dataset = datasets[state]
        #forecasts, tss = get_lag_llama_predictions(dataset, prediction_length=12, device=torch.device("cpu"))

        plt.figure(figsize=(20, 15))
        date_formater = mdates.DateFormatter('%b, %d')
        plt.rcParams.update({'font.size': 15})  # Fixed syntax error

        print(f"RMSE and MSE for {state}:")
        for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):
            ax = plt.subplot(3, 3, idx + 1)
            # Plot actual data and forecasts
            plt.plot(ts[-4 * 12:].to_timestamp(), label="target")
            forecast.plot(color='g')
            plt.xticks(rotation=60)
            ax.xaxis.set_major_formatter(date_formater)

            # Extract actual and forecasted values for RMSE
            actual_values = ts[-forecast.prediction_length:].to_numpy()
            forecast_values = forecast.mean

            # Compute RMSE and MSE
            rmse = calculate_rmse(actual_values, forecast_values)
            mse = rmse ** 2

            # Extract race name
            race_name = forecast.item_id  # Assuming forecast contains item_id

            # Print RMSE and MSE
            print(f"Race: {race_name}, RMSE: {rmse:.4f}, MSE: {mse:.4f}")
            print(forecast_values)

            # Set title with format "State - Race"
            ax.set_title(f"{state} - {race_name}")

        plt.gcf().tight_layout()
        plt.legend()
        plt.show()

plt.figure(figsize=(20, 15))
date_formater = mdates.DateFormatter('%b, %d')
plt.rcParams.update({'font.size': 15})

# Iterate through the first 9 series, and plot the predicted samples
for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):
    ax = plt.subplot(3, 3, idx+1)

    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label="target", )
    forecast.plot( color='g')
    plt.xticks(rotation=60)
    ax.xaxis.set_major_formatter(date_formater)
    ax.set_title(forecast.item_id)

plt.gcf().tight_layout()
plt.legend()
plt.show()
from sklearn.metrics import mean_squared_error
import numpy as np
from tqdm.autonotebook import tqdm


# Function to compute RMSE
def calculate_rmse(actual, predicted):
    return np.sqrt(mean_squared_error(actual, predicted))

# Plot results for each state
states = ["Wyoming"]

for state in states:
    if state in datasets:
        dataset = datasets[state]
        train_dataset = train_datasets[state]

        predictor = estimator.train(train_dataset, cache_data=True, shuffle_buffer_length=1000)
        forecast_it, ts_it = make_evaluation_predictions(
        dataset=train_dataset,
        predictor=predictor,
        num_samples=num_samples
    )
        forecasts = list(tqdm(forecast_it, total=len(dataset), desc="Forecasting batches"))
        tss = list(tqdm(ts_it, total=len(dataset), desc="Ground truth"))


        plt.figure(figsize=(20, 15))
        date_formater = mdates.DateFormatter('%Y,%b, %d')
        plt.rcParams.update({'font.size': 15})  # Fixed syntax error

        print(f"RMSE and MSE for {state}:")
        for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):
            ax = plt.subplot(3, 3, idx + 1)
            # Plot actual data and forecasts
            plt.plot(ts[-4 * 12:].to_timestamp(), label="target")
            forecast.plot(color='g')
            plt.xticks(rotation=60)
            ax.xaxis.set_major_formatter(date_formater)

            # Extract actual and forecasted values for RMSE
            actual_values = ts[-forecast.prediction_length:].to_numpy()
            forecast_values = forecast.mean

            # Compute RMSE and MSE
            rmse = calculate_rmse(actual_values, forecast_values)
            mse = rmse ** 2

            # Extract race name
            race_name = forecast.item_id  # Assuming forecast contains item_id

            # Print RMSE and MSE
            print(f"Race: {race_name}, RMSE: {rmse:.4f}, MSE: {mse:.4f}")
            print("Forecast vlaues")
            print(forecast_values)

            # Set title with format "State - Race"
            ax.set_title(f"{state} - {race_name}")


        plt.gcf().tight_layout()
        plt.legend()
        plt.show()

