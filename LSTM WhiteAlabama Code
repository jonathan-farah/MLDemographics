!pip install datasets
# Import necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from datasets import load_dataset
import matplotlib.pyplot as plt

df = load_dataset("AdityaA44/RacepopulationAlabama", split='train').to_pandas()

df.head()

# Data processing and scaling
# years = df['Year'].values.reshape(-1, 1)
white_alone = df['White'].values.reshape(-1, 1)

# Standardize the data
scaler = StandardScaler()
# years_scaled = scaler.fit_transform(years)
white_alone_scaled = scaler.fit_transform(white_alone)

# Convert to tensors
# years_tensor = torch.tensor(years_scaled, dtype=torch.float32)
white_alone_tensor = torch.tensor(white_alone_scaled, dtype=torch.float32)


def create_dataset(X, window_size):
    num_samples = (X.shape[0] - window_size)
    X_featurized, y = [], []

    for i in range(num_samples):
        X_featurized.append(X[i:i+window_size])
        y.append(X[i+1:i+window_size+1])

    return torch.stack(X_featurized).squeeze(), torch.stack(y).squeeze()

# Split the data based on specific years for training and validation

# X_train, y_train = years_tensor[train_indices], white_alone_tensor[train_indices]
# X_val, y_val = years_tensor[val_indices], white_alone_tensor[val_indices]
X_full, y = create_dataset(white_alone_tensor, 5)
train_size = int(len(X_full) * 0.8)

X_train, y_train = X_full[:train_size], y[:train_size]
X_val, y_val = X_full[train_size:], y[train_size:]

# Convert to TensorDatasets
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)

# Create DataLoaders for batching
batch_size = 4
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
combined_dataset = DataLoader([train_dataset, val_dataset])

# Create a DataLoader for the combined dataset
combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=False)


class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        batch_size = x.size(0)
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)

        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out)
        return out


# Define hyperparameters
input_size = 1  # One feature ('White Alone')
hidden_size = 512
num_layers = 3
output_size = 1  # Predicting one value (population)
num_epochs = 36
learning_rate = 0.0001

# Initialize the model, loss function, and optimizer
model = LSTMModel(input_size, hidden_size, num_layers, output_size)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)


# Initialize the model, loss function, and optimizer
model = LSTMModel(input_size, hidden_size, num_layers, output_size)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch.unsqueeze(-1))  # Add a feature dimension
        loss = criterion(outputs, y_batch.unsqueeze(-1))
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * X_batch.size(0)

    train_loss /= len(train_loader.dataset)

    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            outputs = model(X_batch.unsqueeze(-1))  # Add a feature dimension
            loss = criterion(outputs, y_batch.unsqueeze(-1))
            val_loss += loss.item() * X_batch.size(0)

    val_loss /= len(val_loader.dataset)
    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')

    best_val_loss = float('inf')
best_epoch = 0



# Evaluate the model on the validation set
model.eval()
predictions = []
actuals = []
with torch.no_grad():
    for input in X_full:
        output = model(input.unsqueeze(0).unsqueeze(-1)).squeeze()[-1] # Ensure shape: (1, seq_len, input_size)
        predictions.append(output.item())

# Convert predictions and actuals to original scale
predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))
all_actuals = df['White'].values

# Extract years for the validation set
# test_years = df[df['Year'] >= 1990]['Year'].values

# val_years = df[df['Year'] >= 2020]['Year'].values


# # Plot predictions vs actuals
# plt.figure(figsize=(10, 6))
# plt.plot(val_years, actuals, label='Actual')
# plt.plot(val_years, predictions, label='Predicted')
# plt.title('Actual vs Predicted White Alone Population')
# plt.xlabel('Year')
# plt.ylabel('White Alone Population')
# plt.legend()

# import numpy as np
# import matplotlib.pyplot as plt
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import DataLoader, TensorDataset
# from sklearn.preprocessing import StandardScaler

# # Data processing and scaling
# years = df['Year'].values.reshape(-1, 1)
# white_alone = df['White'].values.reshape(-1, 1)

# # Standardize the data
# scaler_X = StandardScaler()
# scaler_y = StandardScaler()
# years_scaled = scaler_X.fit_transform(years)
# white_alone_scaled = scaler_y.fit_transform(white_alone)

# # Convert to tensors
# years_tensor = torch.tensor(years_scaled, dtype=torch.float32)
# white_alone_tensor = torch.tensor(white_alone_scaled, dtype=torch.float32)

# # Split the data
# train_indices = (df['Year'] >= 1990) & (df['Year'] <= 2019)
# val_indices = (df['Year'] >= 2020) & (df['Year'] <= 2022)

# X_train, y_train = years_tensor[train_indices], white_alone_tensor[train_indices]
# X_val, y_val = years_tensor[val_indices], white_alone_tensor[val_indices]

# # Create DataLoaders
# train_dataset = TensorDataset(X_train, y_train)
# val_dataset = TensorDataset(X_val, y_val)
# batch_size = 32
# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# # Modified LSTM model
# class LSTMModel(nn.Module):
#     def __init__(self, input_size, hidden_size, num_layers, output_size):
#         super(LSTMModel, self).__init__()
#         self.hidden_size = hidden_size
#         self.num_layers = num_layers
#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
#         self.fc = nn.Linear(hidden_size, output_size)

#     def forward(self, x):
#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
#         out, _ = self.lstm(x, (h0, c0))
#         out = self.fc(out[:, -1, :])
#         return out

# Hyperparameters
# input_size = 1
# hidden_size = 64
# num_layers = 2
# output_size = 1
# num_epochs = 1000
# learning_rate = 0.001

# # Initialize model, loss function, and optimizer
# model = LSTMModel(input_size, hidden_size, num_layers, output_size)
# criterion = nn.MSELoss()
# optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
# for epoch in range(num_epochs):
#     model.train()
#     for X_batch, y_batch in train_loader:
#         optimizer.zero_grad()
#         X_batch = X_batch.unsqueeze(1)  # Ensure input has shape (batch_size, seq_len, input_size)
#         outputs = model(X_batch)
#         loss = criterion(outputs, y_batch)
#         loss.backward()
#         optimizer.step()

    # if (epoch + 1) % 100 == 0:
    #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Evaluation
# model.eval()
# all_years_scaled = scaler_X.transform(years)
# all_years_tensor = torch.tensor(all_years_scaled, dtype=torch.float32).unsqueeze(1)  # Shape: (N, 1, 1)
# all_predictions = []

# with torch.no_grad():
#     for year in all_years_tensor:
#         output = model(year.unsqueeze(0))  # Ensure shape: (1, seq_len, input_size)
#         all_predictions.append(scaler_y.inverse_transform(output.numpy()))

# all_predictions = np.array(all_predictions).flatten()
# all_actuals = df['White'].values

# Plotting
plt.figure(figsize=(15, 6))
plt.plot(df['Year'][5:], all_actuals[5:], label='Actual', marker='o')
plt.plot(df['Year'][5:], predictions, label='Predicted', marker='x')
plt.title('Actual vs Predicted White Alone Population (1990-2022)')
plt.xlabel('Year')
plt.ylabel('White Alone Population')
plt.legend()
plt.grid(True)
plt.xticks(range(1990, 2023, 1), rotation=45)
plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f"{int(x)}"))
plt.tight_layout()
plt.show()

# Calculate and print MSE
mse = np.mean((all_actuals - predictions)**2)
print(f"Mean Squared Error: {mse}")
